{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cultivate's processed Enron data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/yuwenwu/insight/cultivate/data/processed/'\n",
    "date_col = ['message__sent_at']\n",
    "sentences_all_cols = pd.read_csv(path + 'enron_case_study_messages.csv', parse_dates = date_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 472230 entries, 0 to 472229\n",
      "Data columns (total 40 columns):\n",
      "sentence_index                  472230 non-null int64\n",
      "sentiment_value                 472230 non-null int64\n",
      "message__id                     472230 non-null object\n",
      "message__chat__id               472230 non-null object\n",
      "message__sent_at                472230 non-null datetime64[ns]\n",
      "message__contact__parent__id    472230 non-null object\n",
      "request_prob                    472089 non-null float64\n",
      "concreteness                    472230 non-null float64\n",
      "politeness                      416681 non-null float64\n",
      "is_question                     472230 non-null bool\n",
      "message__provider_guid          472230 non-null int64\n",
      "message__chat__team__id         472230 non-null object\n",
      "msg_size_x                      472230 non-null int64\n",
      "advs                            472230 non-null int64\n",
      "articles                        472230 non-null int64\n",
      "aux_verbs                       472230 non-null int64\n",
      "convo_index                     472230 non-null int64\n",
      "convo_size                      472230 non-null int64\n",
      "da_label                        472089 non-null object\n",
      "da_prob                         472089 non-null float64\n",
      "imp_prons                       472230 non-null int64\n",
      "msg_size_y                      472230 non-null int64\n",
      "non_stop_word_lemmas            442514 non-null object\n",
      "num_female                      472230 non-null int64\n",
      "num_female_pronouns             472230 non-null int64\n",
      "num_male                        472230 non-null int64\n",
      "num_male_pronouns               472230 non-null int64\n",
      "num_subordinate                 472230 non-null int64\n",
      "num_superior                    472230 non-null int64\n",
      "num_unique_tokens               472230 non-null int64\n",
      "num_unknowngender               472230 non-null int64\n",
      "num_unknownpower                472230 non-null int64\n",
      "parent_id                       15675 non-null object\n",
      "parent_prob                     15675 non-null float64\n",
      "per_prons                       472230 non-null int64\n",
      "preps                           472230 non-null int64\n",
      "quants                          472230 non-null int64\n",
      "response_time                   15675 non-null object\n",
      "sender_gender                   472230 non-null object\n",
      "sub_conjs                       472230 non-null int64\n",
      "dtypes: bool(1), datetime64[ns](1), float64(5), int64(24), object(9)\n",
      "memory usage: 141.0+ MB\n"
     ]
    }
   ],
   "source": [
    "sentences_all_cols.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentiment_value</th>\n",
       "      <th>message__id</th>\n",
       "      <th>message__chat__id</th>\n",
       "      <th>message__sent_at</th>\n",
       "      <th>message__contact__parent__id</th>\n",
       "      <th>request_prob</th>\n",
       "      <th>concreteness</th>\n",
       "      <th>politeness</th>\n",
       "      <th>is_question</th>\n",
       "      <th>message__provider_guid</th>\n",
       "      <th>message__chat__team__id</th>\n",
       "      <th>msg_size_x</th>\n",
       "      <th>advs</th>\n",
       "      <th>articles</th>\n",
       "      <th>aux_verbs</th>\n",
       "      <th>convo_index</th>\n",
       "      <th>convo_size</th>\n",
       "      <th>da_label</th>\n",
       "      <th>da_prob</th>\n",
       "      <th>imp_prons</th>\n",
       "      <th>msg_size_y</th>\n",
       "      <th>non_stop_word_lemmas</th>\n",
       "      <th>num_female</th>\n",
       "      <th>num_female_pronouns</th>\n",
       "      <th>num_male</th>\n",
       "      <th>num_male_pronouns</th>\n",
       "      <th>num_subordinate</th>\n",
       "      <th>num_superior</th>\n",
       "      <th>num_unique_tokens</th>\n",
       "      <th>num_unknowngender</th>\n",
       "      <th>num_unknownpower</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>parent_prob</th>\n",
       "      <th>per_prons</th>\n",
       "      <th>preps</th>\n",
       "      <th>quants</th>\n",
       "      <th>response_time</th>\n",
       "      <th>sender_gender</th>\n",
       "      <th>sub_conjs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5kT1sN1BY08ZfoTYcUHo_Q</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.447116</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.269913</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>westdesk#parsing#california#file#schedules#sch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5kT1sN1BY08ZfoTYcUHo_Q</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.229237</td>\n",
       "      <td>4.465000</td>\n",
       "      <td>0.433185</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>0.345797</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>message#log</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5kT1sN1BY08ZfoTYcUHo_Q</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.148573</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>0.399924</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.309767</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>variance#detect</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5kT1sN1BY08ZfoTYcUHo_Q</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.047411</td>\n",
       "      <td>3.296667</td>\n",
       "      <td>0.389760</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.357122</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>ancillary#hour#schedule#hourahead#award#start#...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5NLpNTSV2Ui1EZdCWFTbAg</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-06 07:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.447116</td>\n",
       "      <td>False</td>\n",
       "      <td>724694</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.269913</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>westdesk#parsing#california#file#schedules#sch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index  sentiment_value             message__id  \\\n",
       "0               3                1  5kT1sN1BY08ZfoTYcUHo_Q   \n",
       "1               2                2  5kT1sN1BY08ZfoTYcUHo_Q   \n",
       "2               1                2  5kT1sN1BY08ZfoTYcUHo_Q   \n",
       "3               0                1  5kT1sN1BY08ZfoTYcUHo_Q   \n",
       "4               3                1  5NLpNTSV2Ui1EZdCWFTbAg   \n",
       "\n",
       "        message__chat__id    message__sent_at message__contact__parent__id  \\\n",
       "0  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26       bM1QDebT6vOFy4ifK_OcGA   \n",
       "1  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26       bM1QDebT6vOFy4ifK_OcGA   \n",
       "2  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26       bM1QDebT6vOFy4ifK_OcGA   \n",
       "3  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26       bM1QDebT6vOFy4ifK_OcGA   \n",
       "4  0EUsnl18IABPqYfgDRHaoA 2001-10-06 07:38:26       bM1QDebT6vOFy4ifK_OcGA   \n",
       "\n",
       "   request_prob  concreteness  politeness  is_question  \\\n",
       "0      0.154175      3.980000    0.447116        False   \n",
       "1      0.229237      4.465000    0.433185        False   \n",
       "2      0.148573      2.160000    0.399924        False   \n",
       "3      0.047411      3.296667    0.389760        False   \n",
       "4      0.154175      3.980000    0.447116        False   \n",
       "\n",
       "   message__provider_guid message__chat__team__id  msg_size_x  advs  articles  \\\n",
       "0                  723973  XZwXgkxlOGssBijnWEMOig           3     0         0   \n",
       "1                  723973  XZwXgkxlOGssBijnWEMOig           3     0         0   \n",
       "2                  723973  XZwXgkxlOGssBijnWEMOig           3     0         0   \n",
       "3                  723973  XZwXgkxlOGssBijnWEMOig           3     0         0   \n",
       "4                  724694  XZwXgkxlOGssBijnWEMOig           3     0         0   \n",
       "\n",
       "   aux_verbs  convo_index  convo_size      da_label   da_prob  imp_prons  \\\n",
       "0          0            0           1        Inform  0.269913          0   \n",
       "1          0            0           1  Conventional  0.345797          0   \n",
       "2          0            0           1        Inform  0.309767          0   \n",
       "3          0            0           1        Inform  0.357122          0   \n",
       "4          0            0           1        Inform  0.269913          0   \n",
       "\n",
       "   msg_size_y                               non_stop_word_lemmas  num_female  \\\n",
       "0           3  westdesk#parsing#california#file#schedules#sch...           0   \n",
       "1           3                                        message#log           0   \n",
       "2           3                                    variance#detect           0   \n",
       "3           3  ancillary#hour#schedule#hourahead#award#start#...           0   \n",
       "4           3  westdesk#parsing#california#file#schedules#sch...           0   \n",
       "\n",
       "   num_female_pronouns  num_male  num_male_pronouns  num_subordinate  \\\n",
       "0                    0         5                  0                0   \n",
       "1                    0         5                  0                0   \n",
       "2                    0         5                  0                0   \n",
       "3                    0         5                  0                0   \n",
       "4                    0         5                  0                0   \n",
       "\n",
       "   num_superior  num_unique_tokens  num_unknowngender  num_unknownpower  \\\n",
       "0             0                 16                  6                11   \n",
       "1             0                  2                  6                11   \n",
       "2             0                  3                  6                11   \n",
       "3             0                 11                  6                11   \n",
       "4             0                 16                  6                11   \n",
       "\n",
       "  parent_id  parent_prob  per_prons  preps  quants response_time  \\\n",
       "0       NaN          NaN          0      0       0           NaN   \n",
       "1       NaN          NaN          0      0       0           NaN   \n",
       "2       NaN          NaN          0      0       0           NaN   \n",
       "3       NaN          NaN          0      0       0           NaN   \n",
       "4       NaN          NaN          0      0       0           NaN   \n",
       "\n",
       "  sender_gender  sub_conjs  \n",
       "0             I          0  \n",
       "1             I          0  \n",
       "2             I          0  \n",
       "3             I          0  \n",
       "4             I          0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_all_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences_all_cols.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_index</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>request_prob</th>\n",
       "      <th>concrete</th>\n",
       "      <th>polite_prob</th>\n",
       "      <th>question</th>\n",
       "      <th>msg_id</th>\n",
       "      <th>team_id</th>\n",
       "      <th>msg_size</th>\n",
       "      <th>adv</th>\n",
       "      <th>articles</th>\n",
       "      <th>aux_verbs</th>\n",
       "      <th>convo_index</th>\n",
       "      <th>convo_size</th>\n",
       "      <th>da_label</th>\n",
       "      <th>da_prob</th>\n",
       "      <th>imp_prons</th>\n",
       "      <th>vocab</th>\n",
       "      <th>female_recipients</th>\n",
       "      <th>female_prons</th>\n",
       "      <th>male_recipients</th>\n",
       "      <th>male_prons</th>\n",
       "      <th>subordinates</th>\n",
       "      <th>superiors</th>\n",
       "      <th>unique_tokens</th>\n",
       "      <th>gender_unknown</th>\n",
       "      <th>power_unknown</th>\n",
       "      <th>parent_predict</th>\n",
       "      <th>parent_prob</th>\n",
       "      <th>per_prons</th>\n",
       "      <th>preps</th>\n",
       "      <th>quants</th>\n",
       "      <th>sender_gender</th>\n",
       "      <th>sub_conjs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.447116</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.269913</td>\n",
       "      <td>0</td>\n",
       "      <td>westdesk#parsing#california#file#schedules#sch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.229237</td>\n",
       "      <td>4.465000</td>\n",
       "      <td>0.433185</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>0.345797</td>\n",
       "      <td>0</td>\n",
       "      <td>message#log</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.148573</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>0.399924</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.309767</td>\n",
       "      <td>0</td>\n",
       "      <td>variance#detect</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.047411</td>\n",
       "      <td>3.296667</td>\n",
       "      <td>0.389760</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.357122</td>\n",
       "      <td>0</td>\n",
       "      <td>ancillary#hour#schedule#hourahead#award#start#...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-06 07:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.447116</td>\n",
       "      <td>False</td>\n",
       "      <td>724694</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.269913</td>\n",
       "      <td>0</td>\n",
       "      <td>westdesk#parsing#california#file#schedules#sch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_index  sentiment                 chat_id           timestamp  \\\n",
       "0           3          1  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26   \n",
       "1           2          2  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26   \n",
       "2           1          2  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26   \n",
       "3           0          1  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26   \n",
       "4           3          1  0EUsnl18IABPqYfgDRHaoA 2001-10-06 07:38:26   \n",
       "\n",
       "                sender_id  request_prob  concrete  polite_prob  question  \\\n",
       "0  bM1QDebT6vOFy4ifK_OcGA      0.154175  3.980000     0.447116     False   \n",
       "1  bM1QDebT6vOFy4ifK_OcGA      0.229237  4.465000     0.433185     False   \n",
       "2  bM1QDebT6vOFy4ifK_OcGA      0.148573  2.160000     0.399924     False   \n",
       "3  bM1QDebT6vOFy4ifK_OcGA      0.047411  3.296667     0.389760     False   \n",
       "4  bM1QDebT6vOFy4ifK_OcGA      0.154175  3.980000     0.447116     False   \n",
       "\n",
       "   msg_id                 team_id  msg_size  adv  articles  aux_verbs  \\\n",
       "0  723973  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "1  723973  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "2  723973  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "3  723973  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "4  724694  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "\n",
       "   convo_index  convo_size      da_label   da_prob  imp_prons  \\\n",
       "0            0           1        Inform  0.269913          0   \n",
       "1            0           1  Conventional  0.345797          0   \n",
       "2            0           1        Inform  0.309767          0   \n",
       "3            0           1        Inform  0.357122          0   \n",
       "4            0           1        Inform  0.269913          0   \n",
       "\n",
       "                                               vocab  female_recipients  \\\n",
       "0  westdesk#parsing#california#file#schedules#sch...                  0   \n",
       "1                                        message#log                  0   \n",
       "2                                    variance#detect                  0   \n",
       "3  ancillary#hour#schedule#hourahead#award#start#...                  0   \n",
       "4  westdesk#parsing#california#file#schedules#sch...                  0   \n",
       "\n",
       "   female_prons  male_recipients  male_prons  subordinates  superiors  \\\n",
       "0             0                5           0             0          0   \n",
       "1             0                5           0             0          0   \n",
       "2             0                5           0             0          0   \n",
       "3             0                5           0             0          0   \n",
       "4             0                5           0             0          0   \n",
       "\n",
       "   unique_tokens  gender_unknown  power_unknown parent_predict  parent_prob  \\\n",
       "0             16               6             11            NaN          NaN   \n",
       "1              2               6             11            NaN          NaN   \n",
       "2              3               6             11            NaN          NaN   \n",
       "3             11               6             11            NaN          NaN   \n",
       "4             16               6             11            NaN          NaN   \n",
       "\n",
       "   per_prons  preps  quants sender_gender  sub_conjs  \n",
       "0          0      0       0             I          0  \n",
       "1          0      0       0             I          0  \n",
       "2          0      0       0             I          0  \n",
       "3          0      0       0             I          0  \n",
       "4          0      0       0             I          0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.drop(['message__id', 'response_time', 'msg_size_y'], axis = 1, inplace = True)\n",
    "cols = ['sent_index', 'sentiment', 'chat_id', 'timestamp',\n",
    "       'sender_id', 'request_prob', 'concrete', 'polite_prob',\n",
    "       'question', 'msg_id', 'team_id', 'msg_size', 'adv',\n",
    "       'articles', 'aux_verbs', 'convo_index', 'convo_size',\n",
    "       'da_label', 'da_prob', 'imp_prons', \n",
    "       'vocab', 'female_recipients', 'female_prons',\n",
    "       'male_recipients', 'male_prons', 'subordinates',\n",
    "       'superiors', 'unique_tokens', 'gender_unknown',\n",
    "       'power_unknown', 'parent_predict', 'parent_prob',\n",
    "       'per_prons', 'preps', 'quants', 'sender_gender', 'sub_conjs']\n",
    "sentences.columns = cols\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-name sender and chat IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_ids = {}\n",
    "sender_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse sentence text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for words in sentences['vocab']:\n",
    "    if type(words) == float:\n",
    "        vocab.append(np.nan)\n",
    "    else:\n",
    "        word_list = words.split('#')\n",
    "        vocab.append(' '.join(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_index</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>chat_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>request_prob</th>\n",
       "      <th>concrete</th>\n",
       "      <th>polite_prob</th>\n",
       "      <th>question</th>\n",
       "      <th>msg_id</th>\n",
       "      <th>team_id</th>\n",
       "      <th>msg_size</th>\n",
       "      <th>adv</th>\n",
       "      <th>articles</th>\n",
       "      <th>aux_verbs</th>\n",
       "      <th>convo_index</th>\n",
       "      <th>convo_size</th>\n",
       "      <th>da_label</th>\n",
       "      <th>da_prob</th>\n",
       "      <th>imp_prons</th>\n",
       "      <th>vocab</th>\n",
       "      <th>female_recipients</th>\n",
       "      <th>female_prons</th>\n",
       "      <th>male_recipients</th>\n",
       "      <th>male_prons</th>\n",
       "      <th>subordinates</th>\n",
       "      <th>superiors</th>\n",
       "      <th>unique_tokens</th>\n",
       "      <th>gender_unknown</th>\n",
       "      <th>power_unknown</th>\n",
       "      <th>parent_predict</th>\n",
       "      <th>parent_prob</th>\n",
       "      <th>per_prons</th>\n",
       "      <th>preps</th>\n",
       "      <th>quants</th>\n",
       "      <th>sender_gender</th>\n",
       "      <th>sub_conjs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.447116</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.269913</td>\n",
       "      <td>0</td>\n",
       "      <td>westdesk parsing california file schedules sch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.229237</td>\n",
       "      <td>4.465000</td>\n",
       "      <td>0.433185</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Conventional</td>\n",
       "      <td>0.345797</td>\n",
       "      <td>0</td>\n",
       "      <td>message log</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.148573</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>0.399924</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.309767</td>\n",
       "      <td>0</td>\n",
       "      <td>variance detect</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-05 02:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.047411</td>\n",
       "      <td>3.296667</td>\n",
       "      <td>0.389760</td>\n",
       "      <td>False</td>\n",
       "      <td>723973</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.357122</td>\n",
       "      <td>0</td>\n",
       "      <td>ancillary hour schedule hourahead award start ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0EUsnl18IABPqYfgDRHaoA</td>\n",
       "      <td>2001-10-06 07:38:26</td>\n",
       "      <td>bM1QDebT6vOFy4ifK_OcGA</td>\n",
       "      <td>0.154175</td>\n",
       "      <td>3.980000</td>\n",
       "      <td>0.447116</td>\n",
       "      <td>False</td>\n",
       "      <td>724694</td>\n",
       "      <td>XZwXgkxlOGssBijnWEMOig</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Inform</td>\n",
       "      <td>0.269913</td>\n",
       "      <td>0</td>\n",
       "      <td>westdesk parsing california file schedules sch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent_index  sentiment                 chat_id           timestamp  \\\n",
       "0           3          1  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26   \n",
       "1           2          2  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26   \n",
       "2           1          2  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26   \n",
       "3           0          1  0EUsnl18IABPqYfgDRHaoA 2001-10-05 02:38:26   \n",
       "4           3          1  0EUsnl18IABPqYfgDRHaoA 2001-10-06 07:38:26   \n",
       "\n",
       "                sender_id  request_prob  concrete  polite_prob  question  \\\n",
       "0  bM1QDebT6vOFy4ifK_OcGA      0.154175  3.980000     0.447116     False   \n",
       "1  bM1QDebT6vOFy4ifK_OcGA      0.229237  4.465000     0.433185     False   \n",
       "2  bM1QDebT6vOFy4ifK_OcGA      0.148573  2.160000     0.399924     False   \n",
       "3  bM1QDebT6vOFy4ifK_OcGA      0.047411  3.296667     0.389760     False   \n",
       "4  bM1QDebT6vOFy4ifK_OcGA      0.154175  3.980000     0.447116     False   \n",
       "\n",
       "   msg_id                 team_id  msg_size  adv  articles  aux_verbs  \\\n",
       "0  723973  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "1  723973  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "2  723973  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "3  723973  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "4  724694  XZwXgkxlOGssBijnWEMOig         3    0         0          0   \n",
       "\n",
       "   convo_index  convo_size      da_label   da_prob  imp_prons  \\\n",
       "0            0           1        Inform  0.269913          0   \n",
       "1            0           1  Conventional  0.345797          0   \n",
       "2            0           1        Inform  0.309767          0   \n",
       "3            0           1        Inform  0.357122          0   \n",
       "4            0           1        Inform  0.269913          0   \n",
       "\n",
       "                                               vocab  female_recipients  \\\n",
       "0  westdesk parsing california file schedules sch...                  0   \n",
       "1                                        message log                  0   \n",
       "2                                    variance detect                  0   \n",
       "3  ancillary hour schedule hourahead award start ...                  0   \n",
       "4  westdesk parsing california file schedules sch...                  0   \n",
       "\n",
       "   female_prons  male_recipients  male_prons  subordinates  superiors  \\\n",
       "0             0                5           0             0          0   \n",
       "1             0                5           0             0          0   \n",
       "2             0                5           0             0          0   \n",
       "3             0                5           0             0          0   \n",
       "4             0                5           0             0          0   \n",
       "\n",
       "   unique_tokens  gender_unknown  power_unknown parent_predict  parent_prob  \\\n",
       "0             16               6             11            NaN          NaN   \n",
       "1              2               6             11            NaN          NaN   \n",
       "2              3               6             11            NaN          NaN   \n",
       "3             11               6             11            NaN          NaN   \n",
       "4             16               6             11            NaN          NaN   \n",
       "\n",
       "   per_prons  preps  quants sender_gender  sub_conjs  \n",
       "0          0      0       0             I          0  \n",
       "1          0      0       0             I          0  \n",
       "2          0      0       0             I          0  \n",
       "3          0      0       0             I          0  \n",
       "4          0      0       0             I          0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences['vocab'] = vocab\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29716 null vocab entries, which constitutes 0.06 of the total sentences.\n"
     ]
    }
   ],
   "source": [
    "null_vocab = sentences['vocab'].isnull().sum()\n",
    "print('There are {0} null vocab entries, which constitutes {1:.02f} of the total sentences.'\\\n",
    "     .format(null_vocab, null_vocab / len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "msg_ids = sentences['msg_id'].tolist()\n",
    "words = sentences['vocab'].tolist()\n",
    "prev_doc_id = sentences['msg_id'][0]\n",
    "doc_words = ''\n",
    "for i in range(len(sentences)):\n",
    "    current_doc_id = msg_ids[i]\n",
    "    current_doc_words = words[i]\n",
    "    if type(current_doc_words) == float:\n",
    "        continue\n",
    "    elif current_doc_id == prev_doc_id:\n",
    "        doc_words = ' '.join([doc_words, current_doc_words])\n",
    "    else:\n",
    "        docs.append(doc_words)\n",
    "        doc_words = current_doc_words\n",
    "        prev_doc_id = current_doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67910"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "68752"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)\n",
    "sentences['msg_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['westdesk parsing california file schedules scheduling iso portland final message log variance detect hour schedule hourahead award start ancillary date',\n",
       " 'westdesk parsing california file schedules scheduling iso portland final message log load variance detect schedule variance detect hour hourahead schedule award start ancillary date',\n",
       " 'westdesk parsing california file schedules scheduling iso portland final message log variance detect hour schedule hourahead award start ancillary date',\n",
       " 'westdesk parsing california file schedules scheduling iso portland final message log variance detect hour schedule hourahead award start ancillary date',\n",
       " 'westdesk parsing california file schedules scheduling iso portland final message log generation variance detect schedule variance detect hour schedule hourahead award start ancillary date',\n",
       " 'westdesk parsing california file schedules scheduling iso portland final message log import/export energy detect schedule variance variance detect ancillary hour schedule hourahead award start date',\n",
       " 'westdesk parsing california file schedules scheduling iso portland final message log variance detect hour schedule hourahead award start ancillary date',\n",
       " 'delaney hectic energy ca regard memo attach future plan finances folk solutions',\n",
       " 'westdesk parsing california file schedules scheduling iso portland final message log variance detect hour schedule hourahead award start ancillary date',\n",
       " 'westdesk parsing california file schedules scheduling iso portland final message log sc variance detect schedule trades import/export energy detect schedule variance variance detect hour schedule hourahead award start ancillary date']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442514"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.93707303644410567"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "472230"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sentences_all_cols['non_stop_word_lemmas'].str.contains('westdesk parsing california file \\\n",
    "schedules scheduling iso portland final message log variance detect \\\n",
    "hour schedule hourahead award start ancillary date').count()\n",
    "\n",
    "s\n",
    "s / len(sentences_all_cols)\n",
    "len(sentences_all_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per message analysis\n",
    "Within a message, how much variation is there in sentiment, etc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = sentences.groupby('msg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_calc = ['sentiment', 'request_prob', 'concrete', 'polite_prob', 'adv', 'articles', 'aux_verbs', 'imp_prons',\n",
    "             'female_recipients', 'female_prons', 'male_recipients', 'male_prons', 'subordinates', 'superiors',\n",
    "             'unique_tokens', 'gender_unknown', 'power_unknown', 'per_prons', 'preps', 'quants', 'sub_conjs']\n",
    "\n",
    "msg_id = messages['sentiment'].mean().index\n",
    "message_means = pd.DataFrame(msg_id, columns = ['msg_id'])\n",
    "message_stds = message_means.copy()\n",
    "for col in cols_calc:\n",
    "    message_means[col] = messages[col].mean().tolist()\n",
    "    message_stds[col] = messages[col].std().tolist()\n",
    "\n",
    "#per_message['sentiment'].mean()\n",
    "#also want num questions per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68752"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "68752"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences['msg_id'].nunique()\n",
    "len(message_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Enron email data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('enron.mbox.json', 'r') as json_data:\n",
    "    emails = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve just the body of the email to clean the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_content = []\n",
    "for i in range(len(emails)):\n",
    "    content = emails[i]['parts'][0]['content']\n",
    "    email_content.append(content)\n",
    "\n",
    "#clean strings\n",
    "for i in range(len(email_content)):\n",
    "    email_content[i] = email_content[i].replace('\\n', ' ')\n",
    "    email_content[i] = email_content[i].replace('\\t', ' ')\n",
    "    email_content[i] = re.sub('--+', '', email_content[i])\n",
    "    email_content[i] = re.sub(' +', ' ', email_content[i])\n",
    "    email_content[i] = re.sub('[0-9]+/[0-9]+/[0-9]+ ([0-9]+:[0-9]+)* [a-zA-Z]{2}', '', email_content[i])\n",
    "    email_content[i] = re.sub('@([a-zA-Z])*', '', email_content[i])\n",
    "    email_content[i] = re.sub('/[a-zA-Z]*/[a-zA-Z]*', '', email_content[i])\n",
    "    email_content[i] = email_content[i].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features from raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps = []\n",
    "total_msg_lens = []\n",
    "periods, commas = [], []\n",
    "ex_marks, q_marks = [], []\n",
    "forwarded = []\n",
    "word_tokens = []\n",
    "pos = []\n",
    "for email in email_content:\n",
    "    caps.append(sum(1 for s in email if s.isupper()))\n",
    "    total_msg_lens.append(len(email))\n",
    "    periods.append(sum(1 for s in email if s == '.'))\n",
    "    commas.append(sum(1 for s in email if s == ','))\n",
    "    ex_marks.append(sum(1 for s in email if s == '!'))\n",
    "    q_marks.append(sum(1 for s in email if s == '?'))\n",
    "    if 'Forwarded' in email:\n",
    "        forwarded.append(True)\n",
    "    else:\n",
    "        forwarded.append(False)\n",
    "    #email = email.lower()\n",
    "    pos.append(nlp(email))\n",
    "    word_tokens.append(word_tokenize(email))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(email_content, 'email_content.pkl');\n",
    "#email_content = joblib.load('email_content.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = string.punctuation\n",
    "word_counts = []\n",
    "for i in range(len(word_tokens)):\n",
    "    words = [w for w in word_tokens[i] if not w in punct]\n",
    "    word_counts.append(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop word removal may eliminate important words in the context of the email. First try without stop word removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords = nltk.corpus.stopwords.words('english')\n",
    "#for i in range(len(word_tokens)):\n",
    " #   word_tokens[i] = [w for w in word_tokens[i] if not w in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(word_tokens, 'word_tokens.pkl')\n",
    "word_tokens = joblib.load('word_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = pd.DataFrame(\n",
    "    {\n",
    "        'caps':caps,\n",
    "        'msg_len':total_msg_lens,\n",
    "        'periods':periods,\n",
    "        'commas':commas,\n",
    "        'ex_marks':ex_marks,\n",
    "        'q_marks':q_marks,\n",
    "        'forwarded':forwarded,\n",
    "        'word_count':word_counts\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caps</th>\n",
       "      <th>commas</th>\n",
       "      <th>ex_marks</th>\n",
       "      <th>forwarded</th>\n",
       "      <th>msg_len</th>\n",
       "      <th>periods</th>\n",
       "      <th>q_marks</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>773</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>219</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>198</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>81</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>1926</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caps  commas  ex_marks  forwarded  msg_len  periods  q_marks  word_count\n",
       "0     1       0         0      False       20        0        0           4\n",
       "1    12       1         0      False      773       10        0         140\n",
       "2     0       0         3      False       29        1        0           5\n",
       "3     6       1         0      False      180        2        0          34\n",
       "4     2       0         0      False       33        1        0           7\n",
       "5     5       1         0      False       56        0        1           9\n",
       "6    16       0         0      False      219        4        0          30\n",
       "7     0       0         0      False       32        0        0           6\n",
       "8    15       0         0      False      198       15        0          32\n",
       "9    81      17         2       True     1926       17        0         299"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(text_features, 'text_features.pkl');\n",
    "text_features = joblib.load('text_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for token in word_tokens:\n",
    "    text.append(' '.join(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_texts(texts):\n",
    "    docs = []\n",
    "    for doc in nlp.pipe(texts, n_threads = 4):\n",
    "        \n",
    "        # Iterate over base NPs, e.g. \"all their good ideas\"\n",
    "        tokens = []\n",
    "        for np_ in list(doc.noun_chunks):\n",
    "            \n",
    "            # Only keep adjectives and nouns, e.g. \"good ideas\"\n",
    "            nps = []\n",
    "            try:\n",
    "                while len(np_) >= 1 and np_[0].dep_ in ('amod', 'compound'):\n",
    "                    np_ = np_[1:]\n",
    "                if len(np_) >= 1:\n",
    "                    # Merge the tokens, e.g. good_ideas\n",
    "                    nps.append(np_.merge(np_.root.tag_, np_.text, np_.root.ent_type_))\n",
    "            except IndexError:\n",
    "                pass\n",
    "            # Iterate over named entities\n",
    "            ents = []\n",
    "            for ent in doc.ents:\n",
    "                if len(ent) > 1:\n",
    "                    # Merge them into single tokens\n",
    "                    ents.append(ent.merge(ent.root.tag_, ent.text, ent.label_))\n",
    "            \n",
    "            token_strings = []\n",
    "            for np_ in nps:\n",
    "                text = np_.text.replace(' ', '_')\n",
    "                tag = np_.ent_type_ or np_.tag_\n",
    "                token_strings.append('{}|{}'.format(text, tag))\n",
    "                tokens.append(' '.join(token_strings))\n",
    "                tokens = list(set(tokens))\n",
    "        \n",
    "        if not tokens:\n",
    "            docs.append([])\n",
    "        else:\n",
    "            docs.append(tokens)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 13s, sys: 4.94 s, total: 11min 18s\n",
      "Wall time: 11min 37s\n"
     ]
    }
   ],
   "source": [
    "%time docs_capitalized_punct = transform_texts(text)\n",
    "pickle.dump(docs_capitalized_punct, open('docs_capitalized_punct.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['PRP', 'WP', 'PERSON']\n",
    "docs_filtered = []\n",
    "for doc in docs_capitalized_punct:\n",
    "    if not doc:\n",
    "        continue\n",
    "    term_list = []\n",
    "    for term in doc:\n",
    "        if not any(s in term for s in to_remove):\n",
    "            term_list.append(term)\n",
    "    docs_filtered.append(term_list)\n",
    "#pickle.dump(docs_filtered, open('docs_capitalized_punct_filtered.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_filtered = pickle.load(open('docs_capitalized_punct_filtered.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"activities|NNS\" + 0.012*\"e|NN\" + 0.011*\"Brent|ORG\" + 0.011*\"regard|NN\" + 0.010*\"Thanks|NNS\"'),\n",
       " (1,\n",
       "  '0.010*\"Subject|NN\" + 0.007*\"I_\\'m|NN\" + 0.006*\"Dallas|GPE\" + 0.006*\"the_tickets|NNS\" + 0.006*\"the_detail|NN\"'),\n",
       " (2,\n",
       "  '0.017*\"Calgary|GPE\" + 0.011*\"the_rest|NN\" + 0.009*\"a_lot|NN\" + 0.009*\"Portland|GPE\" + 0.009*\"your_input|NN\"'),\n",
       " (3,\n",
       "  '0.063*\"http|NN\" + 0.025*\"this_request|NN\" + 0.021*\"Name|NN\" + 0.016*\"ID|NN\" + 0.015*\"this_email|NN\"'),\n",
       " (4,\n",
       "  '0.016*\"offices|NNS\" + 0.016*\"http|NN\" + 0.012*\"Director|NNP\" + 0.010*\">|NNP\" + 0.009*\"night|TIME\"'),\n",
       " (5,\n",
       "  '0.009*\"operations|NNS\" + 0.008*\"London|GPE\" + 0.007*\"the_location|NN\" + 0.006*\"consistency|NN\" + 0.006*\"knowledge|NN\"'),\n",
       " (6,\n",
       "  '0.011*\"the_one|NN\" + 0.009*\"Thanks|NNS\" + 0.009*\"discussion|NN\" + 0.008*\"my_cell_phone|NN\" + 0.008*\"the_top|NN\"'),\n",
       " (7,\n",
       "  '0.010*\"message|NN\" + 0.006*\"gas|NN\" + 0.006*\"June|DATE\" + 0.005*\"nobody|NN\" + 0.005*\"Subject|NN\"'),\n",
       " (8,\n",
       "  '0.017*\"Thanks|NNS\" + 0.007*\"To|NNP\" + 0.007*\"personnel|NNS\" + 0.006*\"traders|NNS\" + 0.005*\"a_report|NN\"'),\n",
       " (9,\n",
       "  '0.010*\"Subject|NN\" + 0.007*\"the_report|NN\" + 0.007*\"light|NN\" + 0.007*\"players|NNS\" + 0.007*\"Manager|NNP\"'),\n",
       " (10,\n",
       "  '0.014*\"EES|ORG\" + 0.013*\"schedule|NN\" + 0.012*\"San_Diego|GPE\" + 0.011*\"action|NN\" + 0.011*\"room|NN\"'),\n",
       " (11,\n",
       "  '0.006*\"interest|NN\" + 0.006*\"info|NN\" + 0.006*\"offers|NNS\" + 0.005*\"Thanks|NNS\" + 0.005*\"management|NN\"'),\n",
       " (12,\n",
       "  '0.029*\"http|NN\" + 0.022*\"this_email|NN\" + 0.019*\"email|NN\" + 0.017*\"employees|NNS\" + 0.009*\"Nothing|NN\"'),\n",
       " (13,\n",
       "  '0.017*\"Congratulations|NNS\" + 0.012*\"your_promotion|NN\" + 0.008*\"your_note|NN\" + 0.006*\"Regards|NNS\" + 0.005*\"Thanks|NNS\"'),\n",
       " (14,\n",
       "  '0.016*\"Corp.|NNP\" + 0.009*\"my_calendar|NN\" + 0.007*\"contacts|NNS\" + 0.007*\"content|NN\" + 0.006*\"that_date|NN\"'),\n",
       " (15,\n",
       "  '0.015*\"controls|NNS\" + 0.011*\"this_process|NN\" + 0.010*\"this_information|NN\" + 0.008*\"practices|NNS\" + 0.008*\"books|NNS\"'),\n",
       " (16,\n",
       "  '0.007*\"RAC|ORG\" + 0.005*\"the_United_States|GPE\" + 0.005*\"charges|NNS\" + 0.005*\"advice|NN\" + 0.005*\"the_review|NN\"'),\n",
       " (17,\n",
       "  '0.021*\"work|NN\" + 0.016*\"Subject|NN\" + 0.016*\"To|NNP\" + 0.010*\"care|NN\" + 0.009*\"the_ball|NN\"'),\n",
       " (18,\n",
       "  '0.020*\"distribution|NN\" + 0.019*\"anyone|NN\" + 0.018*\"use|NN\" + 0.018*\"the_message|NN\" + 0.017*\"others|NNS\"'),\n",
       " (19,\n",
       "  '0.004*\"http|NN\" + 0.003*\"the_season|NN\" + 0.003*\"the_season|DATE\" + 0.002*\"time|NN\" + 0.002*\"Dallas|GPE\"'),\n",
       " (20,\n",
       "  '0.004*\"the_creation|NN\" + 0.004*\"the_way|NN\" + 0.003*\"this_newsletter|NN\" + 0.003*\"the_world|NN\" + 0.003*\"the_delivery|NN\"'),\n",
       " (21,\n",
       "  '0.018*\"Thanks|NNS\" + 0.011*\"meeting|NN\" + 0.009*\"activity|NN\" + 0.008*\"any_questions|NNS\" + 0.007*\"deals|NNS\"'),\n",
       " (22,\n",
       "  '0.101*\"Message|NN\" + 0.041*\"To|NN\" + 0.036*\"Subject|NN\" + 0.025*\"PM_To|LANGUAGE\" + 0.025*\"RE|NN\"'),\n",
       " (23,\n",
       "  '0.010*\"the_data|NNS\" + 0.009*\"a_result|NN\" + 0.008*\"that_time|NN\" + 0.008*\"concerns|NNS\" + 0.008*\"products|NNS\"'),\n",
       " (24,\n",
       "  '0.022*\"town|NN\" + 0.014*\"next_week|DATE\" + 0.009*\"Subject|NN\" + 0.008*\"note|NN\" + 0.006*\"these_changes|NNS\"'),\n",
       " (25,\n",
       "  '0.017*\"Enron|ORG\" + 0.011*\"the_company|NN\" + 0.009*\"trading|NN\" + 0.008*\"this_group|NN\" + 0.008*\"President|NNP\"'),\n",
       " (26,\n",
       "  '0.005*\"these_meetings|NNS\" + 0.004*\"the_timing|NN\" + 0.003*\"a_review|NN\" + 0.003*\"direction|NN\" + 0.003*\"any_information|NN\"'),\n",
       " (27,\n",
       "  '0.016*\"experience|NN\" + 0.008*\"resources|NNS\" + 0.007*\"my_name|NN\" + 0.007*\"an_issue|NN\" + 0.005*\"curves|NNS\"'),\n",
       " (28,\n",
       "  '0.037*\"Thanks|NNS\" + 0.034*\"Enron_North_America_Corp.|ORG\" + 0.027*\"Subject|NN\" + 0.020*\"any_questions|NNS\" + 0.018*\"everyone|NN\"'),\n",
       " (29,\n",
       "  '0.016*\"some_time|NN\" + 0.015*\"process|NN\" + 0.010*\"teams|NNS\" + 0.010*\"groups|NNS\" + 0.009*\"people|NNS\"'),\n",
       " (30,\n",
       "  '0.030*\"dinner|NN\" + 0.015*\"Saturday|DATE\" + 0.014*\"Subject|NN\" + 0.010*\"football|NN\" + 0.010*\"dates|NNS\"'),\n",
       " (31,\n",
       "  '0.018*\"ENA|ORG\" + 0.013*\"the_Chairman|NNP\" + 0.010*\"location|NN\" + 0.008*\"Enron_North_America|ORG\" + 0.008*\"discussions|NNS\"'),\n",
       " (32,\n",
       "  '0.015*\"touch|NN\" + 0.014*\"the_link|NN\" + 0.012*\"Enron_North_America_Corp.|ORG\" + 0.011*\"advance|NN\" + 0.011*\"Sydney|GPE\"'),\n",
       " (33,\n",
       "  '0.007*\"the_content|NN\" + 0.007*\"Comments|NNS\" + 0.007*\"the_right|NN\" + 0.007*\"the_system|NN\" + 0.006*\"your_calendar|NN\"'),\n",
       " (34,\n",
       "  '0.010*\"Subject|NN\" + 0.009*\"Details|NNS\" + 0.009*\"the_trip|NN\" + 0.008*\"Florida|GPE\" + 0.008*\"the_draft|NN\"'),\n",
       " (35,\n",
       "  '0.005*\"students|NNS\" + 0.005*\"areas|NNS\" + 0.004*\"reservations|NNS\" + 0.003*\"funds|NNS\" + 0.003*\"the_road|NN\"'),\n",
       " (36,\n",
       "  '0.010*\"your_order|NN\" + 0.010*\"documentation|NN\" + 0.008*\"luck|NN\" + 0.008*\"the_page|NN\" + 0.008*\"April|DATE\"'),\n",
       " (37,\n",
       "  '0.011*\"processes|NNS\" + 0.011*\"Thanks|NNS\" + 0.010*\"a_message|NN\" + 0.010*\"SAP|ORG\" + 0.009*\"your_request|NN\"'),\n",
       " (38,\n",
       "  '0.011*\"Enron|ORG\" + 0.010*\"Subject|NN\" + 0.010*\"an_e-mail|NN\" + 0.010*\"Thanks|NNS\" + 0.008*\"accounting|NN\"'),\n",
       " (39,\n",
       "  '0.017*\"the_London_office|NN\" + 0.016*\"your_group|NN\" + 0.007*\"emails|NNS\" + 0.007*\"messages|NNS\" + 0.006*\"system|NN\"'),\n",
       " (40,\n",
       "  '0.009*\"confirmations|NNS\" + 0.008*\"attendees|NNS\" + 0.008*\"information|NN\" + 0.008*\"the_schedule|NN\" + 0.007*\"volumes|NNS\"'),\n",
       " (41,\n",
       "  '0.032*\"Thanks|NNS\" + 0.024*\"the_office|NN\" + 0.021*\"vacation|NN\" + 0.017*\"Sunday|DATE\" + 0.012*\"Monday|DATE\"'),\n",
       " (42,\n",
       "  '0.019*\"the_team|NN\" + 0.017*\"the_game|NN\" + 0.014*\"Thanks|NNS\" + 0.014*\"Enron|ORG\" + 0.012*\"responsibility|NN\"'),\n",
       " (43,\n",
       "  '0.008*\"any_questions|NNS\" + 0.008*\"the_role|NN\" + 0.007*\"the_remainder|NN\" + 0.006*\"case|NN\" + 0.006*\"the_importance|NN\"'),\n",
       " (44,\n",
       "  '0.032*\"London|GPE\" + 0.021*\"Houston|GPE\" + 0.012*\"Thanks|NNS\" + 0.008*\"Brent|NNP\" + 0.007*\"Europe|LOC\"'),\n",
       " (45,\n",
       "  '0.019*\"the_week|DATE\" + 0.010*\"Everyone|NN\" + 0.008*\"the_allocation|NN\" + 0.008*\"offense|NN\" + 0.008*\"Chicago|GPE\"'),\n",
       " (46,\n",
       "  '0.009*\"group|NN\" + 0.008*\"testing|NN\" + 0.007*\"commodities|NNS\" + 0.006*\"ve|NN\" + 0.006*\"reason|NN\"'),\n",
       " (47,\n",
       "  '0.013*\"questions|NNS\" + 0.009*\"Houston|GPE\" + 0.009*\"transactions|NNS\" + 0.007*\"requests|NNS\" + 0.007*\"participants|NNS\"'),\n",
       " (48,\n",
       "  '0.008*\"http|NN\" + 0.006*\"this_link|NN\" + 0.006*\"the_book|NN\" + 0.005*\"information|NN\" + 0.005*\"every_effort|NN\"'),\n",
       " (49,\n",
       "  '0.035*\"lunch|NN\" + 0.017*\"Subject|NN\" + 0.017*\"Thanks|NNS\" + 0.013*\"a_call|NN\" + 0.010*\"the_day|DATE\"')]"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel_cap_punct_filtered.print_topics(num_topics = num_topics, num_words = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this model was trained on corpus where nouns were forced to lowercase and punctuation was removed; seems to perform pretty well\n",
    "#if use this one need to re-save raw data\n",
    "#pickle.dump(ldamodel, open('ldamodel_nocap_nopunct.p', 'wb'))\n",
    "#ldamodel.print_topics(num_topics=num_topics, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate distribution of terms to remove most common and most frequent ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(335027, 99412)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [item for sublist in docs_filtered for item in sublist]\n",
    "len(vocab), len(list(set(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(int)\n",
    "for word in vocab:\n",
    "    d[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2245\n",
       "dtype: int64"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([d])\n",
    "df = df.T\n",
    "df = df.drop(df[0].idxmax())\n",
    "df.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x231875320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = df[(df[0] > 5) & (df[0] < 100)]\n",
    "df2.hist(bins = 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8432"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_to_keep = []\n",
    "for k, v in d.items():\n",
    "    if v > 5 and v < 100:\n",
    "        terms_to_keep.append(k)\n",
    "\n",
    "len(terms_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_to_keep = {key: None for key in terms_to_keep}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter documents by terms that appeared at least 5 times but fewer than 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_common_terms = []\n",
    "empty_indices_common = []\n",
    "for idx, doc in enumerate(docs_filtered):\n",
    "    term_list = []\n",
    "    for term in doc:\n",
    "        if term in terms_to_keep:\n",
    "            term_list.append(term)\n",
    "    if not term_list:\n",
    "        empty_indices_common.append(idx)\n",
    "    else:\n",
    "        docs_common_terms.append(term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(docs_common_terms, open('docs_common_terms.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(docs_common_terms)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in docs_common_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 31s, sys: 2.66 s, total: 6min 34s\n",
      "Wall time: 6min 40s\n"
     ]
    }
   ],
   "source": [
    "num_topics = 50\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "%time ldamodel_common = lda(doc_term_matrix, num_topics = num_topics, id2word = dictionary, passes = 50)\n",
    "#pickle.dump(ldamodel_common, open('ldamodel_common.p', 'wb'))\n",
    "ldamodel_common = pickle.load(open('ldamodel_common.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.041*\"next_week|DATE\" + 0.035*\"your_support|NN\" + 0.033*\"this_meeting|NN\" + 0.023*\"Please_click_http|NN\" + 0.023*\"form|NN\"'),\n",
       " (1,\n",
       "  '0.043*\"SAP|ORG\" + 0.025*\"these_deals|NNS\" + 0.024*\"contacts|NNS\" + 0.023*\"staff|NN\" + 0.018*\"an_update|NN\"'),\n",
       " (2,\n",
       "  '0.052*\"your_promotion|NN\" + 0.049*\"Director|NNP\" + 0.036*\"availability|NN\" + 0.030*\"directors|NNS\" + 0.029*\"the_web|NN\"'),\n",
       " (3,\n",
       "  '0.047*\"messages|NNS\" + 0.032*\"March|DATE\" + 0.029*\"the_students|NNS\" + 0.025*\"these_reports|NNS\" + 0.025*\"an_effort|NN\"'),\n",
       " (4,\n",
       "  '0.021*\"requests|NNS\" + 0.021*\"the_memo|NN\" + 0.021*\"response|NN\" + 0.021*\"a_draft|NN\" + 0.019*\"entities|NNS\"'),\n",
       " (5,\n",
       "  '0.014*\"practice|NN\" + 0.013*\"the_season|NN\" + 0.011*\"Global_Products|ORG\" + 0.009*\"a_presentation|NN\" + 0.009*\"noon|TIME\"'),\n",
       " (6,\n",
       "  '0.034*\"discussion|NN\" + 0.026*\"the_case|NN\" + 0.020*\"the_end|NN\" + 0.018*\"that_meeting|NN\" + 0.017*\"an_opportunity|NN\"'),\n",
       " (7,\n",
       "  '0.037*\"your_review|NN\" + 0.037*\"your_name|NN\" + 0.034*\"RAC|ORG\" + 0.033*\"school|NN\" + 0.030*\"group|NN\"'),\n",
       " (8,\n",
       "  '0.049*\"settlements|NNS\" + 0.041*\"contract|NN\" + 0.037*\"users|NNS\" + 0.034*\"the_issue|NN\" + 0.025*\"our_customers|NNS\"'),\n",
       " (9,\n",
       "  '0.011*\"the_ball|NN\" + 0.010*\"the_start|NN\" + 0.008*\"Your_site|NN\" + 0.008*\"these_e-reports|NNS\" + 0.008*\"SportsLine.com_Inc|ORG\"'),\n",
       " (10,\n",
       "  '0.039*\"reporting|NN\" + 0.037*\"the_note|NN\" + 0.029*\"AA|GPE\" + 0.021*\"IT|NN\" + 0.020*\"the_table|NN\"'),\n",
       " (11,\n",
       "  '0.044*\"discussions|NNS\" + 0.041*\"practices|NNS\" + 0.040*\"regards|NNS\" + 0.033*\"anybody|NN\" + 0.026*\"my_schedule|NN\"'),\n",
       " (12,\n",
       "  '0.024*\"your_cooperation|NN\" + 0.023*\"your_message|NN\" + 0.022*\"your_response|NN\" + 0.021*\"the_top|NN\" + 0.017*\"the_groups|NNS\"'),\n",
       " (13,\n",
       "  '0.030*\"notice|NN\" + 0.030*\"board|NN\" + 0.029*\"accounting|NN\" + 0.028*\"my_resume|NN\" + 0.024*\"Details|NNS\"'),\n",
       " (14,\n",
       "  '0.036*\"development|NN\" + 0.031*\"North_America|LOC\" + 0.029*\"offices|NNS\" + 0.025*\"assistance|NN\" + 0.022*\"needs|NNS\"'),\n",
       " (15,\n",
       "  '0.043*\"the_presentation|NN\" + 0.034*\"Love|WORK_OF_ART\" + 0.030*\"the_info|NN\" + 0.022*\"drinks|NNS\" + 0.020*\"names|NNS\"'),\n",
       " (16,\n",
       "  '0.022*\"the_business_units|NNS\" + 0.021*\"San_Francisco|GPE\" + 0.019*\"lots|NNS\" + 0.018*\"friends|NNS\" + 0.016*\"every_effort|NN\"'),\n",
       " (17,\n",
       "  '0.027*\"#|NN\" + 0.027*\"a_manager|NN\" + 0.026*\"the_morning|TIME\" + 0.025*\"this_event|NN\" + 0.024*\"a_team|NN\"'),\n",
       " (18,\n",
       "  '0.037*\"standards|NNS\" + 0.028*\"the_role|NN\" + 0.019*\"this_effort|NN\" + 0.016*\"Florida|GPE\" + 0.015*\"lines|NNS\"'),\n",
       " (19,\n",
       "  '0.034*\"last_year|DATE\" + 0.032*\"this_memo|NN\" + 0.030*\"counterparties|NNS\" + 0.027*\"an_approver|NN\" + 0.024*\"<_<_File|NN\"'),\n",
       " (20,\n",
       "  '0.027*\"Tokyo|GPE\" + 0.024*\"B|NN\" + 0.024*\"the_importance|NN\" + 0.020*\"analysts|NNS\" + 0.017*\"the_scope|NN\"'),\n",
       " (21,\n",
       "  '0.033*\"cost|NN\" + 0.027*\"the_past|NN\" + 0.026*\"December|DATE\" + 0.025*\"the_attachment|NN\" + 0.024*\"roles|NNS\"'),\n",
       " (22,\n",
       "  '0.025*\"functions|NNS\" + 0.022*\"confidence|NN\" + 0.018*\"a_series|NN\" + 0.017*\"commodities|NNS\" + 0.016*\"discussions|NNS\"'),\n",
       " (23,\n",
       "  '0.047*\"Enron_North_America|ORG\" + 0.043*\"today|DATE\" + 0.029*\"call|NN\" + 0.026*\"a_report|NN\" + 0.024*\"his_role|NN\"'),\n",
       " (24,\n",
       "  '0.041*\"morning|TIME\" + 0.038*\"ENA|NN\" + 0.034*\"ASAP|ORG\" + 0.031*\"Fernley|ORG\" + 0.028*\"Dad|NNP\"'),\n",
       " (25,\n",
       "  '0.044*\"Sydney|GPE\" + 0.044*\"the_request|NN\" + 0.020*\"system|NN\" + 0.018*\"the_direction|NN\" + 0.018*\"participants|NNS\"'),\n",
       " (26,\n",
       "  '0.049*\"attention|NN\" + 0.034*\"A|NNP\" + 0.032*\"the_site|NN\" + 0.030*\"the_remainder|NN\" + 0.023*\"the_transition|NN\"'),\n",
       " (27,\n",
       "  '0.043*\"any_comments|NNS\" + 0.029*\"teams|NNS\" + 0.026*\"your_approval|NN\" + 0.025*\"implementation|NN\" + 0.024*\"location|NN\"'),\n",
       " (28,\n",
       "  '0.040*\"SAP|NNP\" + 0.032*\"help|NN\" + 0.031*\"Australia|GPE\" + 0.028*\"the_names|NNS\" + 0.025*\"travel|NN\"'),\n",
       " (29,\n",
       "  '0.037*\"games|NNS\" + 0.031*\"Everyone|NN\" + 0.026*\"your_calendars|NNS\" + 0.024*\"progress|NN\" + 0.023*\"presentation|NN\"'),\n",
       " (30,\n",
       "  '0.056*\"message|NN\" + 0.046*\"the_data|NNS\" + 0.040*\"your_comments|NNS\" + 0.034*\"this_project|NN\" + 0.031*\"the_integrity|NN\"'),\n",
       " (31,\n",
       "  '0.032*\"team|NN\" + 0.026*\"charge|NN\" + 0.025*\"managers|NNS\" + 0.020*\"Canada|GPE\" + 0.019*\"documentation|NN\"'),\n",
       " (32,\n",
       "  '0.018*\"performance|NN\" + 0.017*\"results|NNS\" + 0.016*\"The_company|NN\" + 0.012*\"the_standards|NNS\" + 0.011*\"production|NN\"'),\n",
       " (33,\n",
       "  '0.033*\"knowledge|NN\" + 0.028*\"regard|NN\" + 0.026*\"a_problem|NN\" + 0.024*\"your_assistance|NN\" + 0.024*\"my_group|NN\"'),\n",
       " (34,\n",
       "  '0.027*\"the_report|NN\" + 0.027*\"analysis|NN\" + 0.026*\"Attached|NNP\" + 0.022*\"copies|NNS\" + 0.021*\"MSN_Hotmail|ORG\"'),\n",
       " (35,\n",
       "  '0.043*\"the_London_office|NN\" + 0.019*\"life|NN\" + 0.019*\"Hope|NN\" + 0.016*\"consistency|NN\" + 0.016*\"recommendations|NNS\"'),\n",
       " (36,\n",
       "  '0.032*\"the_issues|NNS\" + 0.030*\"Operations|ORG\" + 0.029*\"input|NN\" + 0.027*\"the_program|NN\" + 0.027*\"a_summary|NN\"'),\n",
       " (37,\n",
       "  '0.033*\"Calgary|GPE\" + 0.026*\"EBS|ORG\" + 0.021*\"the_organization|NN\" + 0.020*\"Portland|GPE\" + 0.018*\"concern|NN\"'),\n",
       " (38,\n",
       "  '0.030*\"Japan|GPE\" + 0.025*\"your_group|NN\" + 0.018*\"Singapore|GPE\" + 0.017*\"ext|NN\" + 0.016*\"officials|NNS\"'),\n",
       " (39,\n",
       "  '0.039*\"EES|ORG\" + 0.032*\"a_job|NN\" + 0.031*\"luck|NN\" + 0.027*\"a_change|NN\" + 0.020*\"the_next_few_days|DATE\"'),\n",
       " (40,\n",
       "  '0.056*\"a_position|NN\" + 0.025*\"football|NN\" + 0.024*\"activity|NN\" + 0.023*\"these_changes|NNS\" + 0.020*\"any_interest|NN\"'),\n",
       " (41,\n",
       "  '0.029*\"content|NN\" + 0.021*\"Mom|NN\" + 0.021*\"list|NN\" + 0.018*\"our_efforts|NNS\" + 0.013*\"permission|NN\"'),\n",
       " (42,\n",
       "  '0.026*\"San_Diego|GPE\" + 0.021*\"your_time|NN\" + 0.019*\"a_date|NN\" + 0.017*\"all_locations|NNS\" + 0.016*\"Atlanta|GPE\"'),\n",
       " (43,\n",
       "  '0.031*\"history|NN\" + 0.025*\"authority|NN\" + 0.023*\"reservations|NNS\" + 0.021*\"the_hotel|NN\" + 0.018*\"last_week|DATE\"'),\n",
       " (44,\n",
       "  '0.055*\"Brent|ORG\" + 0.046*\"contact|NN\" + 0.038*\"the_trip|NN\" + 0.037*\"a_while|NN\" + 0.030*\"My_assistant|NN\"'),\n",
       " (45,\n",
       "  '0.021*\"responsibilities|NNS\" + 0.021*\"Brent|NNP\" + 0.018*\"review|NN\" + 0.015*\"South_America|LOC\" + 0.013*\"any_way|NN\"'),\n",
       " (46,\n",
       "  '0.028*\"the_draft|NN\" + 0.021*\"your_input|NN\" + 0.016*\"your_family|NN\" + 0.015*\"coverage|NN\" + 0.013*\"ve|NN\"'),\n",
       " (47,\n",
       "  '0.032*\"phone|NN\" + 0.029*\"my_calendar|NN\" + 0.023*\"RC|ORG\" + 0.020*\"limits|NNS\" + 0.018*\"base|NN\"'),\n",
       " (48,\n",
       "  '0.026*\"your_participation|NN\" + 0.026*\"students|NNS\" + 0.025*\"campus|NN\" + 0.024*\"the_success|NN\" + 0.023*\"a_letter|NN\"'),\n",
       " (49,\n",
       "  '0.042*\"e|NN\" + 0.030*\"locations|NNS\" + 0.027*\"the_value|NN\" + 0.022*\"the_schedule|NN\" + 0.022*\"recognition|NN\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel_common.print_topics(num_topics=num_topics, num_words=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_assignments = ldamodel_common[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(topic_assignments, open('topic_assignments_intermediate.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "probas = []\n",
    "for doc in topic_assignments:\n",
    "    topic, proba = max(doc, key = lambda x:x[1])\n",
    "    topics.append(topic)\n",
    "    probas.append(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get empty indices\n",
    "empty = []\n",
    "for idx, doc in enumerate(docs_capitalized_punct):\n",
    "    if not doc:\n",
    "        empty.append(idx)\n",
    "        continue\n",
    "    term_list = []\n",
    "    tmp = []\n",
    "    for term in doc:\n",
    "        if term in terms_to_keep:\n",
    "            tmp.append(term)\n",
    "    for term in tmp:\n",
    "        if not any(s in term for s in to_remove):\n",
    "            term_list.append(term)\n",
    "    if not term_list:\n",
    "        empty.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12017"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty) + len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = {key:None for key in empty}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(empty, open('empty_indices.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [None] * len(email_content)\n",
    "for i in range(len(l)):\n",
    "    if i in empty:\n",
    "        l[i] = 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = l.copy()\n",
    "index = 0\n",
    "for i in range(len(l)):\n",
    "    if l[i] == None:\n",
    "        l[i] = topics[index]\n",
    "        l2[i] = probas[index]\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features['topic'] = l\n",
    "text_features['proba'] = l2\n",
    "text_features.replace('nan', np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caps</th>\n",
       "      <th>commas</th>\n",
       "      <th>ex_marks</th>\n",
       "      <th>forwarded</th>\n",
       "      <th>msg_len</th>\n",
       "      <th>periods</th>\n",
       "      <th>q_marks</th>\n",
       "      <th>word_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>773</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.287143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.336667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caps  commas  ex_marks  forwarded  msg_len  periods  q_marks  word_count  \\\n",
       "0     1       0         0      False       20        0        0           4   \n",
       "1    12       1         0      False      773       10        0         140   \n",
       "2     0       0         3      False       29        1        0           5   \n",
       "3     6       1         0      False      180        2        0          34   \n",
       "4     2       0         0      False       33        1        0           7   \n",
       "\n",
       "   topic     proba  \n",
       "0    NaN       NaN  \n",
       "1   14.0  0.287143  \n",
       "2    NaN       NaN  \n",
       "3   10.0  0.336667  \n",
       "4    NaN       NaN  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Engineer POS features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_counts = []\n",
    "for doc in pos:\n",
    "    d = defaultdict(int)\n",
    "    for word in doc:\n",
    "        d[word.pos_] +=1\n",
    "    pos_counts.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(pos_counts, open('pos_counts.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj</th>\n",
       "      <th>adp</th>\n",
       "      <th>adv</th>\n",
       "      <th>cconj</th>\n",
       "      <th>det</th>\n",
       "      <th>intj</th>\n",
       "      <th>noun</th>\n",
       "      <th>num</th>\n",
       "      <th>part</th>\n",
       "      <th>pron</th>\n",
       "      <th>propn</th>\n",
       "      <th>punct</th>\n",
       "      <th>space</th>\n",
       "      <th>sym</th>\n",
       "      <th>verb</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    adj   adp   adv  cconj   det  intj  noun  num  part  pron  propn  punct  \\\n",
       "0   1.0   0.0   1.0    0.0   0.0   0.0   1.0  0.0   0.0   0.0    0.0    0.0   \n",
       "1  10.0  13.0  15.0    7.0  16.0   0.0  35.0  0.0   5.0   5.0    1.0   11.0   \n",
       "2   1.0   0.0   0.0    0.0   0.0   0.0   2.0  0.0   1.0   0.0    0.0    4.0   \n",
       "3   2.0   5.0   0.0    2.0   4.0   0.0   9.0  0.0   1.0   2.0    4.0    5.0   \n",
       "4   0.0   2.0   0.0    0.0   0.0   0.0   0.0  1.0   0.0   1.0    1.0    1.0   \n",
       "\n",
       "   space  sym  verb    x  \n",
       "0    0.0  0.0   1.0  0.0  \n",
       "1    0.0  0.0  33.0  0.0  \n",
       "2    0.0  0.0   1.0  0.0  \n",
       "3    0.0  0.0   5.0  0.0  \n",
       "4    0.0  0.0   2.0  0.0  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pos_counts = pd.DataFrame(pos_counts)\n",
    "df_pos_counts.columns = df_pos_counts.columns.str.lower()\n",
    "df_pos_counts.replace(np.nan, 0, inplace = True)\n",
    "df_pos_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caps</th>\n",
       "      <th>commas</th>\n",
       "      <th>ex_marks</th>\n",
       "      <th>forwarded</th>\n",
       "      <th>msg_len</th>\n",
       "      <th>periods</th>\n",
       "      <th>q_marks</th>\n",
       "      <th>word_count</th>\n",
       "      <th>topic</th>\n",
       "      <th>proba</th>\n",
       "      <th>adj</th>\n",
       "      <th>adp</th>\n",
       "      <th>adv</th>\n",
       "      <th>cconj</th>\n",
       "      <th>det</th>\n",
       "      <th>intj</th>\n",
       "      <th>noun</th>\n",
       "      <th>num</th>\n",
       "      <th>part</th>\n",
       "      <th>pron</th>\n",
       "      <th>propn</th>\n",
       "      <th>punct</th>\n",
       "      <th>space</th>\n",
       "      <th>sym</th>\n",
       "      <th>verb</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>773</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.287143</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>180</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.336667</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   caps  commas  ex_marks  forwarded  msg_len  periods  q_marks  word_count  \\\n",
       "0     1       0         0      False       20        0        0           4   \n",
       "1    12       1         0      False      773       10        0         140   \n",
       "2     0       0         3      False       29        1        0           5   \n",
       "3     6       1         0      False      180        2        0          34   \n",
       "4     2       0         0      False       33        1        0           7   \n",
       "\n",
       "   topic     proba   adj   adp   adv  cconj   det  intj  noun  num  part  \\\n",
       "0    NaN       NaN   1.0   0.0   1.0    0.0   0.0   0.0   1.0  0.0   0.0   \n",
       "1   14.0  0.287143  10.0  13.0  15.0    7.0  16.0   0.0  35.0  0.0   5.0   \n",
       "2    NaN       NaN   1.0   0.0   0.0    0.0   0.0   0.0   2.0  0.0   1.0   \n",
       "3   10.0  0.336667   2.0   5.0   0.0    2.0   4.0   0.0   9.0  0.0   1.0   \n",
       "4    NaN       NaN   0.0   2.0   0.0    0.0   0.0   0.0   0.0  1.0   0.0   \n",
       "\n",
       "   pron  propn  punct  space  sym  verb    x  \n",
       "0   0.0    0.0    0.0    0.0  0.0   1.0  0.0  \n",
       "1   5.0    1.0   11.0    0.0  0.0  33.0  0.0  \n",
       "2   0.0    0.0    4.0    0.0  0.0   1.0  0.0  \n",
       "3   2.0    4.0    5.0    0.0  0.0   5.0  0.0  \n",
       "4   1.0    1.0    1.0    0.0  0.0   2.0  0.0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features = pd.concat([text_features, df_pos_counts], axis = 1)\n",
    "text_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process for LDA, sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used Gensim, and trained the model using the Skip-Gram with Negative Sampling algorithm, using a frequency threshold of 10 and 5 iterations. After training, we applied a further frequency threshold of 50, to reduce the run-time memory requirements (see [here](https://explosion.ai/blog/sense2vec-with-spacy))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying pairs\n",
    "\n",
    "* How often did the pair communicate (how many messages)? Could eventually ask how many were new conversations vs. old, man not matter\n",
    "* Timestamp of communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "received_from = {}\n",
    "for email in emails:\n",
    "    key = email['From']\n",
    "    if 'To' not in email:\n",
    "        received_from[key] = []\n",
    "    elif key in received_from:\n",
    "        received_from[key].append(email['To'])\n",
    "    else:\n",
    "        received_from[key] = [email['To']]\n",
    "\n",
    "for k, v in received_from.items():\n",
    "    received_from[k] = [item for sublist in v for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_to = {}\n",
    "for email in emails:\n",
    "    if 'To' in email:\n",
    "        keys = email['To']\n",
    "        for key in keys:\n",
    "            if key in sent_to:\n",
    "                sent_to[key].append(email['From'])\n",
    "            else:\n",
    "                sent_to[key] = [email['From']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tim.belden@enron.com',\n",
       " 'tiffany.miller@enron.com',\n",
       " 'christi.nicolay@enron.com',\n",
       " 'kim.ward@enron.com',\n",
       " 'frank.hayden@enron.com',\n",
       " 'tim.heizenrader@enron.com',\n",
       " 'paul.kaufman@enron.com',\n",
       " 'rebecca.cantrell@enron.com',\n",
       " 'rebecca.cantrell@enron.com',\n",
       " 'sarah.novosel@enron.com',\n",
       " 'philip.polsky@enron.com',\n",
       " 'lisa.jacobson@enron.com',\n",
       " 'alyse.herasimchuk@enron.com',\n",
       " 'john.arnold@enron.com',\n",
       " 'john.arnold@enron.com',\n",
       " 'ina.rangel@enron.com',\n",
       " 'kenny.soignet@enron.com',\n",
       " 'john.arnold@enron.com',\n",
       " 'john.lavorato@enron.com',\n",
       " 'john.arnold@enron.com',\n",
       " 'david.forster@enron.com',\n",
       " 'kenny.ha@enron.com',\n",
       " 'william.kasemervisz@enron.com',\n",
       " 'kimberly.brown@enron.com',\n",
       " 'kenny.ha@enron.com',\n",
       " 'john.lavorato@enron.com',\n",
       " 'cheryl.kuehl@enron.com',\n",
       " 'ted.bland@enron.com',\n",
       " 'beverly.stephens@enron.com',\n",
       " 'david.delainey@enron.com',\n",
       " 'kay.chapman@enron.com',\n",
       " 'kay.chapman@enron.com',\n",
       " 'nicki.daw@enron.com',\n",
       " 'brenda.whitehead@enron.com',\n",
       " 'janette.elbertson@enron.com',\n",
       " 'john.lavorato@enron.com',\n",
       " 'ted.bland@enron.com',\n",
       " 'david.delainey@enron.com',\n",
       " 'veronica.espinoza@enron.com',\n",
       " 'veronica.espinoza@enron.com',\n",
       " 'michelle.vitrella@enron.com',\n",
       " 'paul.lebeau@enron.com',\n",
       " 'jeffrey.gossett@enron.com',\n",
       " 'larry.may@enron.com',\n",
       " 'veronica.espinoza@enron.com',\n",
       " 'nicki.daw@enron.com']"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "philip_allen = emails[0]['From']\n",
    "#received_from[philip_allen]\n",
    "sent_to[philip_allen]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan of attack\n",
    "\n",
    "**Identify pairs**\n",
    "\n",
    "* First get sender/receiver pairs from To/From fields\n",
    "* Build a graph? Visualize it?\n",
    "\n",
    "**Features to identify relationships**\n",
    "\n",
    "* Stylistic\n",
    "    * Count number of punctuations in each message, then remove them. First remove extraneous punctuation?\n",
    "    * Count # capitalizations vs no capitalization and then coerce to lowercase?\n",
    "    * POS?\n",
    "* Speed of responses and/or number of independent communication threads\n",
    "* Semantic (based on sentence, averaged over message)\n",
    "* Topic? Use LDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
